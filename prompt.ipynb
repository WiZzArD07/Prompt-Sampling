import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import matplotlib.pyplot as plt

def top_p_sampling(logits, p=0.9):
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)
    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)

    # Mask out tokens with cumulative probability above threshold p
    sorted_mask = cumulative_probs > p
    if torch.any(sorted_mask):
        sorted_mask[torch.where(sorted_mask)[0][0] + 1:] = True  # Keep up to first above-p token
    sorted_logits[sorted_mask] = float('-inf')

    # Sample from the filtered distribution
    probs = torch.softmax(sorted_logits, dim=-1)
    sampled_index = torch.multinomial(probs, 1)
    return sorted_indices[sampled_index], sorted_logits, probs, sorted_indices

# Load model and tokenizer
model_name = "gpt2"
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)
model.eval()

prompt = "The future of AI is"
inputs = tokenizer(prompt, return_tensors="pt")
input_ids = inputs["input_ids"]

# Get logits for the next token
with torch.no_grad():
    outputs = model(input_ids)
    logits = outputs.logits[0, -1, :]

# Apply Top-p sampling
sampled_token, filtered_logits, probs, sorted_indices = top_p_sampling(logits, p=0.9)

# Decode token
sampled_word = tokenizer.decode(sampled_token)

# Visualize top tokens
top_k = 20
top_tokens = sorted_indices[:top_k]
top_probs = probs[:top_k].detach().numpy()
top_words = [tokenizer.decode([idx]) for idx in top_tokens]

plt.figure(figsize=(10, 5))
plt.bar(top_words, top_probs)
plt.xticks(rotation=45)
plt.title(f"Top-p Sampling Distribution (p=0.9)\nSampled: '{sampled_word}'")
plt.ylabel("Probability")
plt.show()
